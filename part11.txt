<big><strong>Given a sufficiently high n value, any [math]O(n\ log\ n)[/math] algorithm will outperform any [math]O(n^2)[/math] algorithm. When n is small, the reverse is often the case.</strong> In such cases, there will exist (loosely) a critical n value, below which the slower algorithm is faster. This fact can be exploited to optimize performance -- sometimes saving a surprising amount of actual execution time.

Recall this simplified conceptual model:</big>

<a href="http://stickybits.azurewebsites.net/wp-content/uploads/2014/08/threeCurvesTransparent1.png"><img class="aligncenter size-full wp-image-1041" src="http://stickybits.azurewebsites.net/wp-content/uploads/2014/08/threeCurvesTransparent1.png" alt="threeCurvesTransparent" width="575" height="385" /></a>

These curves represent three categories of functions that can appear in a big O expression. Quadratic time [math]O(n^2)[/math] is a "steepening" class algorithm, while linearithmic time [math]O(n\ log\ n)[/math] is what I'm calling a "proportional" class algorithm. We are taking advantage of the smaller n values at the left of the plotting, where proportional algorithms tend to be more expensive than steepening algorithms.

Here are those two functions:



These two curves can be interpreted to quantify the number of constant-time steps required to get the job done. However, each curve plots its function without regard for the time cost of each of these steps. Plotting the curves adjusted for the actual size of those constant-time steps produces kinky curves that vary by actual implementation and only have meaning at integer n values. I'll simplify by using an estimated mean step size for insertion sort (top of part 9) and Quicksort (bottom of part 10). This produces a smooth curve at approximately the right vertical offset and stretch distortion. Let's estimate that for small n values, Quicksort averages about four times more work per inner loop than does insertion sort. Here's how the execution times plot, vertically stretched accordingly:





This raises an important point. In order for the curves to cross, the steepening algorithm must perform less work per innermost iteration and less overhead (setup and conclusion) work than the proportional algorithm. Real data produces much more chaotic performance profiles than these smooth, well behaved curves. This is a simplified model illustrating the expected execution time, given some stochastic data set of size n. Given this simplified model, can you identify the ideal n value on the graph to switch between insertion sort and Quicksort?

<pre>function hybridSort(array)
  return inserSort(array) if (array.length < some_critical_value )
  return qSort(array, 0, array.length-1)
 
function qSort(array, lo, hi)
    return array unless lo < hi
    [innerLo, innerHi] = dutchFlagify(array, lo, hi)
    qInserSort(array, lo, innerLo-1)
    qInserSort(array, innerHi+1, hi)
    return array</pre>

You might be tempted to choose the n value where the two lines cross as the ideal critical value. Let's explore that thought. Using Quicksort for n > 20 and insertion sort for n < 20 produces a combined execution profile like so:

<pre>function hybridSort(array)
  return qInserSort(array, 0, array.length-1)

function qInserSort(array, lo, hi)
  return inserSort(array, lo, hi) unless lo + some_critical_value < hi
  [innerLo, innerHi] = dutchFlagify(array, lo, hi)
  qInserSort(array, lo, innerLo-1)
  qInserSort(array, innerHi+1, hi)
  return array
</pre>




That gives a dramatic improvement when n is near 10, but otherwise very little savings, and identical performance in most cases. Essentially, we get savings when n is too small to care about, and only sometimes at that. A better hybridization would be to do a large number of small insertion sorts, multiplying the savings by that large number:




Now we get the dramatic improvement whenever a recursion of qInserSort is less than the critical value. In effect, we get the savings of the naive interpretation with every return from the base case in the recursion. But now instead of a base case where tiny arrays of length 0 or 1 are too short to be out-of-sequence, our base case is with small arrays where insertion sort is optimal over quicksort.


Here's a more performant version:

<pre>function hybridSort(array)
  coarseQSort(array, 0, array.length-1)
  return inserSort(array)

function coarseQSort(array, lo, hi)
  return unless lo + some_critical_value < hi
  [innerLo, innerHi] = dutchFlagify(array, lo, hi)
  qInserSort(array, lo, innerLo-1)
  qInserSort(array, innerHi+1, hi)
  return array
</pre>

By running a single insertion sort against the entire semi-quicksorted array, we save the overhead of calling and setting up many small insertion sorts, while the insertion crawls are the same regardless.

Let's see this new hybrid algorithm's simplified execution time profile:




Now we have a much flatter curve, providing much higher sorting throughput. This hybrid algorithm can handle much bigger arrays with the same resources as a pure quicksort, and much, much bigger arrays than a pure insertion sort.

It turns out that the ideal critical value is not where the two curves cross, but where their derivatives cross. The best place to make the handoff is the place where the curves are furthest apart, which is also the place where the steepness of the two curves matches. Here is a conceptual diagram of that point in the two functions.




In other words, the place where the per-n workload is furthest apart, the windfall pulls the right side of the curve down the furthest, and if this can happen many times along the spectrum of n values, the curve is substantially flattened.
