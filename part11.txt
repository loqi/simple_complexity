<a title="Simple Complexity" href="http://stickybits.azurewebsites.net/?p=41">jump to beginning of series</a><a title="Simple Complexity 10" href="http://stickybits.azurewebsites.net/?p=2701"> or previous page</a>

<big><strong>Given a sufficiently high n value, any [math]O(n\ log\ n)[/math] algorithm will outperform any [math]O(n^2)[/math] algorithm. When n is small, the reverse is often the case.</strong> In such cases, there will exist (loosely) a critical n value, below which the slower algorithm is faster and the faster algorithm is slower. This fact can be exploited to optimize performance -- sometimes saving a sizable amount of actual execution time.

Insertion sort runs in quadratic time [math]O(n^2)[/math] in the "steepening" class. Quicktime runs in linearithmic time [math]O(n\ log\ n)[/math] which is quasi-linear. We can take advantage of the way their performance profiles cross at the left side of the plotting, where linear algorithms tend to be more expensive than steepening algorithms.

<a href="http://stickybits.azurewebsites.net/wp-content/uploads/2015/01/insertionVsQuick.png"><img src="http://stickybits.azurewebsites.net/wp-content/uploads/2015/01/insertionVsQuick.png" alt="insertionVsQuick" width="625" height="440" class="aligncenter size-full wp-image-13601" /></a>

These curves attempt to naively plot the actual computational work load (vertical axis) required to complete insertion sort (blue curve), and Quicksort (red curve) for any given array length (horizontal axis). Real data produces much more chaotic performance profiles than these smooth, well behaved curves. This is a simplified model that does not attempt to account for all possible differences in the initial data, and only roughly averages out differences in inner-loop workload and setup housekeeping by multiplying and adding constants of dubious applicability. In fact, I chose numbers a bit arbitrarily to widen the curves for illustration. And of course, only integers are meaningful as n values. Even so, this is a useful illustration for this discussion.

For ideal execution performance, one would use insertion sort for smaller n values and Quicksort for bigger. In this radically simplified mathematical model, can you identify the ideal n value along the horizontal axis at which to make that switch?

Here's some code we might write.

<pre>function hybridSort(array)
  return quickSort(array) if (some_critical_value > array.length)
  return insertionSort(array, 0, array.length-1)

function qSort(array, lo, hi)
    return array unless lo < hi
    [innerLo, innerHi] = dutchFlagify(array, lo, hi)
    qSort(array, lo, innerLo-1)
    qSort(array, innerHi+1, hi)
    return array</pre>

You might be tempted to choose the n value where the two lines cross as the ideal critical value. This is actually an ideal choice for the naive code above. At any n value, you always get the better of the two. Using Quicksort for length 20 or more, and insertion sort for length 20 or less produces a combined execution profile like this:

<a href="http://stickybits.azurewebsites.net/wp-content/uploads/2014/11/hybridInserQuickFirst.png"><img src="http://stickybits.azurewebsites.net/wp-content/uploads/2014/11/hybridInserQuickFirst.png" alt="hybridInserQuickFirst" width="625" height="450" class="aligncenter size-full wp-image-13651" /></a>

This barely yields any execution benefit over bare Quicksort, and only when n is too small to care about. We get no improvement at all for big n values. A better hybridization would be to do a large number of small insertion sorts, multiplying the savings by that large number:

<pre>function hybridSort(array)
  return qInserSort(array, 0, array.length-1)

function qInserSort(array, lo, hi)
  return inserSort(array, lo, hi) if lo + some_critical_value > hi
  [innerLo, innerHi] = dutchFlagify(array, lo, hi)
  qInserSort(array, lo, innerLo-1)
  qInserSort(array, innerHi+1, hi)
  return array
</pre>

What would be the critical n value at which to do the handoff?

It's not 20. In this simplified example, it's actually more like 5 or 10, where the vertical difference is widest. This has the effect of shifting the red line down by that amount of vertical distance. But it doesn't just do that once; for a big n value, we get this small improvement many times. In effect we repeat the savings every time we touch the recursion's new base case. The new base case is no longer an n of 0 or 1 where the array is too short to need sorting, but now it's where the array is short enough where insertion sort substantially outperforms Quicksort.

It turns out that the ideal critical value is not where the two curves cross, but where their derivatives cross. The best time to make the handoff is when the savings is biggest -- the place where the steepness of the two curves match. Here's the new simplified execution profile:

Now we have a flatter curve, providing higher throughput. This hybrid algorithm can handle much bigger arrays with the same resources as pure Quicksort, and much, much bigger arrays than a pure insertion sort. This windfall pulls the right side of the curve down the furthest, and if this can happen many times along the spectrum of n values, the curve is substantially flattened. Imagine the naive implementation at the top of this page being invoked against a large number of small arrays. Small n values are not too small to care about if you run the algorithm a large number of times. That is effectively what we have here.

Here's a generally more performant version.

<pre>function hybridSort(array)
  almostQuicksort(array, 0, array.length-1, 12)
  return inserSort(array)

function almostQuicksort(array, lo, hi, mesh)
  return if lo + mesh >= hi
  [innerLo, innerHi] = dutchFlagify(array, lo, hi)
  almostQuicksort(array, lo, innerLo-1)
  almostQuicksort(array, innerHi+1, hi)
  return array
</pre>

By running a single insertion sort against the entire Quicksort13 array, we save the overhead of calling and setting up many small insertion sorts, while the length of the insertion crawls are the same regardless of one long or many short invocations. The only time this arrangement would be less performant is when there are many long runs of equal pivot values (middle Dutch flag stripes), which must be walked across to get to the next run of unsorted elements. Here the critical value `mesh` is hard-coded as 12. That puts the transition typically somewhere between [math]hi-lo+1 = 13[/math] e.g 102..114 width 13, and [math]hi-lo = (13-1)/2 + 1 = 7[/math] e.g. 102..106 width 7. Since a partition can be unbalanced, the possible width of the hot zone array[lo..hi] could be anywhere in the 0..13 range, with a bias toward the 7..13 range.

This performance profile might be visualized like this:

<a href="http://stickybits.azurewebsites.net/wp-content/uploads/2015/01/hybridInserQuickSecond.png"><img src="http://stickybits.azurewebsites.net/wp-content/uploads/2015/01/hybridInserQuickSecond.png" alt="hybridInserQuickSecond" width="631" height="325" class="aligncenter size-full wp-image-13771" /></a>

It is important to be aware of what we're looking at here. This diagram is not meant to represent time complexity, but a radically simplified model of actual execution time. The alternating colors are meant to convey alternating execution flow between insertion sort and Quicksort as n grows big. The actual execution flow doesn't follow this pattern at all. This illustration is more in the realm of visual art than mathematically rigorous function plotting. The main point it's supposed to show is that choosing a good critical handoff value will give you a substantially shallower climb to complete the hybrid sort. There's a balance to be struck between choosing too coarse and too fine a mesh value. A good value will have a dramatic effect when your n value is huge, a bad value will save your algorithm work, but will be less than ideal, and at worst will slightly slow down your hybrid algorithm vs. a pure linearithmic algorithm. Don't believe this illustration too much. It's exaggerated, simplified, and out of scale. But do remember it as a useful informal visual representation of what combining two algorithms can accomplish.

There's another way to understand the execution time of this insertion-Quicksort hybrid. Insertion sort is a quadratic-time algorithm. Insertion sort of thirteen elements can be thought of as a constant-time operation, [math]O(13^2) = O(1)[/math]  This mini insertion sort operation has an actual execution time of, let's say on average [math]10^2 + 10 = 110[/math] arbitrarily units of CPU time.

Quicksort is a linearithmic-time algorithm. This "lazy Quicksort 13", which sorts everything to within thirteen of its final position also runs in linearithmic time, but its actual execution time is about one third as long as pure Quicksort that brings everything to its exact final position. [math]log_2\ 7\ \approx\ 2.8[/math] [math]log_2\ 10\ \approx\ 3.3[/math] [math]log_2\ 13\ \approx\ 3.7[/math] Let's say this Quciksort13 goes down to a resolution of about ten on average, and and uses [math]2n/3.5\ log_2\ (n/10) + 40[/math] of our arbitrary units of CPU time. [The constants in this expression are meant to simulate very rough estimates of differences in setup work at various stages of recursion. They can be adjusted up or down for similar results.]

Combining these two we get roughly [math]220n/3.5\ 110log_2\ (n/10)\ +\ 50[/math] units of CPU time for the hybrid sort, vs. pure Quicksort [math]2n\ log_2\ n + 50[/math]. That puts an n of [math]10^{6}[/math] at a cost of    for pure quicksort vs.     for the hybrid example; and [math]10^{7}[/math] at a cost of     vs.       .



When writing general-purpose code for use in many unforeseeable circumstances, such as a popular library, you could carefully analyze the code, but you'll probably get more reliable results with less effort through empirical performance testing. That's when you write test code to use your code and check the performance metrics as you vary the initial conditions, such as what kind of data is in the sort array, and what possible sequences it might arrive in, and most crucially, what's your mesh value to hand off to insertion sort. Such test code can be automated and randomized in various ways to expose performance characteristics you may not have thought to investigate.

However most of the code we write is not worthy of such treatment. It's generally more trouble than it's worth to rigorously probe the performance profile of our creations. Most of the time we rely only on our intuition. Our intuition can be trained to see what's important and ignore the rest. When you do test your intuition, be prepared to be surprised about actual performance testing. It's extra work to do, but every so often do it anyway. It can pay off in tuning whatever piece of code you're working on, but the greater value is in tuning your ongoing intuition.

<big>We have come to the end of the Simple Complexity mini textbook. There's a lot more to algorithmic complexity analysis, but what you've learned here is plenty for the vast majority of situations real software engineers see day-to-day. If you understood most of this material, you're way ahead on this topic.

The next page is a set of exercises and puzzles to put your knowledge to the test.
</big>

&nbsp;
