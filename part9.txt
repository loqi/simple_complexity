<a title="Simple Complexity" href="http://stickybits.azurewebsites.net/?p=41">jump to beginning of series</a><a title="Simple Complexity 8" href="http://stickybits.azurewebsites.net/?p=6761"> or previous page</a>

<big>Back on page 4 regarding [math]O(n^2)[/math] I said, "little operations and fragments of operations are still operations, so in fact you are doing <em>something</em> exactly [math]n^2[/math] number of times." Attentive readers will have noticed I was playing a bit loose with my apples and oranges: "Running this algorithm against ten times more data will cause it to do about a hundred times more <em>half-compares,</em> and a hundred times more <em>tenth-of-a-swap</em> operations."

<strong>It's time I come clean with my tiny little fib.</strong></big>

<pre>1 function insertionSort(array)
2   for unsortedIndex in 1..(array.length-1)
3       rCursor = unsortedIndex
4       lCursor = rCursor-1
5       element = array[rCursor]
6       while rCursor > 0 and array[lCursor] > element
7           array[rCursor--] = array[lCursor--]
8       array[rCursor] = element
9   return array</pre>

As with selection sort, insertion sort follows a triangular execution pattern. The outer loop on line 2 considers each element except the first, walking forward to the right until it runs out of elements. The inner loop at line 6 walks backward through the sorted region (everything to the left of the outer loop's index) copying whatever it touches to the right by one position until it discovers the new home for the element. The effect is that all elements visited by the inner loop have been shifted, except the last one it visited, which is duplicated. Line 8 overwrites one of those duplicates with the currently considered element and then the outer loop advances to consider the next element in need of a new home.

We can see that if the array is already sorted, line 7 cannot be reached and line 8 overwrites each element with itself. If the array arrives in reverse order, the loop at line 6 iterates <em>unsortedIndex</em> number of times. This is a triangular algorithm. At most, line 7 runs once, then twice, then three times, and so on until the outer loop completes. Triangular algorithms run in [math]O(n^2)[/math] quadratic time -- same as square algorithms.

But Really?

Let's look at how many times line 7 is run. Given the input array that arrives in reverse-sorted order, this table shows the line numbers in the sequence they are executed.

<table>
<tr><th>input array</th><th>line execution sequence</th></tr>
<tr><td>[]</td><td><small>1 2 9</small></td></tr>
<tr><td>[12]</td><td><small>1 2 9</small></td></tr>
<tr><td>[12, 10]</td><td><small>1 2 3 4 5 6 <strong>7 6</strong> 8 2 9</small></td></tr>
<tr><td>[12, 10, 9]</td><td><small>1 2 3 4 5 6 <strong>7 6</strong> 8 2 3 4 5 6 <strong>7 6 7 6</strong> 8 2 9</small></td></tr>
<tr><td>[12, 10, 9, 7]</td><td><small>1 2 3 4 5 6 <strong>7 6</strong> 8 2 3 4 5 6 <strong>7 6 7 6</strong> 8 2 3 4 5 6 <strong>7 6 7 6 7 6</strong> 8 2 9</small></td></tr>
</table>

If we run this code against an array of 101 reverse-sorted elements, line 7 gets executed exactly 5050 times, lines 3 4 5 8 exactly 100 times, and line 9 exactly once.

This line execution pattern looks like a polynomial to me. We can aproximate the number of lines executed something like this, where n is the length of the array minus one:

[math]0.5n^2 + 56n + 3[/math]

For an n of 1000, it would look more like this:

[math]0.5n^2 + 506n + 3[/math]

Of course, this is not really how math works. When n is known to be 100 or 1000 these two polynomials are constants:

[math]0.5(100^2) + 56(100) + 3 = 5000 + 5600 + 3 = 10,603[/math]
[math]0.5(1000)^2 + 506(1000) + 3 = 500,000 + 506,000 + 3 = 1,006,003[/math]

We want a general function to apply to all n values, not a different function for every n. What are these expressions meant to quantify anyway? Number of lines of code executed? Not all lines are created equal. And what's it all for? Time complexity doesn't care how big each step is, only how many of them there are. But why?

<strong>Here's where I revert to the more standard way of explaining big O.</strong> Only the highest-degree term of a polynomial makes a significant contribution as n approaches infinity.

We see this graphically in the table of line execution above. The iterative lines of the inner loop are in bold; The top two lines contain no bold at all, and as n increases, we see progressively more bold in the table. When the input array is ten reverse-sorted elements, execution flow looks like this:

<small>1 2 3 4 5 6 <strong>7 6</strong>
8 2 3 4 5 6 <strong>7 6 7 6</strong>
8 2 3 4 5 6 <strong>7 6 7 6 7 6</strong>
8 2 3 4 5 6 <strong>7 6 7 6 7 6 7 6</strong>
8 2 3 4 5 6 <strong>7 6 7 6 7 6 7 6 7 6</strong>
8 2 3 4 5 6 <strong>7 6 7 6 7 6 7 6 7 6 7 6</strong>
8 2 3 4 5 6 <strong>7 6 7 6 7 6 7 6 7 6 7 6 7 6</strong>
8 2 3 4 5 6 <strong>7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6</strong>
8 2 3 4 5 6 <strong>7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6</strong>
8 2 9</small>

Here, about two thirds of the line numbers are in bold, meaning that only a third of the lines are executed outside the innermost loop. Each time we add another array element, we add another [math]6[/math] non-bolds and another [math]2(n-1)[/math] bolds.

<small>8 2 3 4 5 6 <strong>7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6 7 6</strong></small>

When running this sort against a 1,000,001-length reverse-sorted array, we'd have 6,000,003 non-bold line numbers plus another 1,000,001,000,000 bold line numbers, or 0.0006% of the lines executed are outside the inner loop. The precision of these numbers is too fine to bother with. Do we really care about the difference between those three extra lines out of six million? Or those seven million lines out of a trillion? If you prefer the English long scale, it's called a billion. If you're a math/science type it's unambiguously called [math]10^{12}[/math] or if you're a stickler for nine significant digits it's [math]1.00000700 \times 10^{12}[/math]. Nobody does that. It's just [math]10^{12}[/math]

At small n values, we don't usually care about time complexity, since execution time approaches a small constant as n approaches zero. We only care when n gets big enough to cause us trouble. In mathematics, we talk about n approaching infinity, but in computer science, we hear that as n getting big enough to make us wait impractically long for our algorithm to complete execution.

<big><strong>Convergence</strong></big>

The exact formula for triangular numbers is [math]n(n+1)/2[/math] and for square numbers is [math]n^2[/math] Let plot them both.




In big O, they're both [math]O(n^2)[/math]

&nbsp;
