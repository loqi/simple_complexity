<a title="Simple Complexity" href="http://stickybits.azurewebsites.net/?p=41">jump to beginning of series</a><a title="Simple Complexity 2" href="http://stickybits.azurewebsites.net/?p=291"> or the previous page</a>

<big><strong>Time complexity is not the same as execution time.</strong>  That bears repeating from the previous page as a tipoff to this trick question.</big>

What is the time complexity of this pseudocode?
<pre>loop 1000 times
     loop 1000 times
          sleep 1 second</pre>
This is a <em>constant-time</em> algorithm. All the values are constants. Regardless of data, it sleeps for a million seconds -- twelve days -- every time it's allowed to run all the way through. Just because it's [math]O(1)[/math] doesn't mean it's fast. It means it's consistent in its running time, however long that happens to be. Technically, it means all possible pathways through the code are themselves constant-time operations, but that's a bit circular for now. We'll circle back later.

Let's change something.
<pre>loop N times
     loop 10 times
          sleep 1 second</pre>
In this little snippet, when N is 1 the code causes a sleep for ten seconds, and when N is 2 it sleeps for twenty seconds. Doubling N will always double the sleep time. Any change in the N value has a proportional effect on execution time. This one is a <em>linear time</em> algorithm: [math]O(n)[/math]. The expression in this big O uses lowercase <em>n.</em> If it's obvious what we mean, the prevailing convention is to use either uppercase <em>N</em> or lowercase <em>n</em> as we see fit. If it's not so obvious, we use an unambiguous expression such as [math]O(log\ key.length)[/math] instead. Math would like to be your friend. If you don't yet feel comfortable with such expressions, you soon will.

Now consider this code.
<pre>loop N times
     loop N times
          sleep 1 second</pre>
Here, doubling N will quadruple the sleep time. When N is ten times bigger, our tiny function sleeps for 100 times longer. That makes it [math]O(n^2)[/math] or <em>quadratic time,</em> the littlest of <em>polynomial time.</em>

Our pseudocode could be refactored like so:
<pre>sleep (N^2) seconds</pre>
Notice that the expressions inside the parentheses above matches the big O expression. This is the essence of big O notation. Its inner expression quantifies the number of <em>operations</em> which must be performed in order to complete the task. Whether we implement it as three lines or just one, the effect is the same, so both versions of the code have [math]O(n^2)[/math] time complexity.

You might be asking, how could one invocation of the sleep function represent more than one operation. On a later page, I'll go into more detail on just what one of these "operation" things is that big O tries to quantify. For now, just know that the sleep function is itself an algorithm with time complexity of its own. Someone else wrote that sleep function, but if it does what it's supposed to, it runs in <em>linear time:</em> [math]O(x)[/math], where [math]x[/math] is, you know, [math]N^2[/math], just like our invocation requested.

We could say that one-liner runs in quadratic time [math]O(n^2)[/math] where [math]n = N[/math] or linear time [math]O(n)[/math] where [math]n = N^2[/math] or constant time [math]O(1)[/math] when N is a known constant. Our perspective can be framed along the [math]n[/math] we find the most interesting, and we can leverage one framing to find another. We don't need to know how the code is structured to know that its total run time grows as a square of N. There doesn't need to exist a piece of code that runs in linear time with respect to N in order for us to think of our code as performing N number of sleep(N) operations. All that matters is that we find it useful to think in those terms.

Let's apply this tool to code we can see.
<pre>1. loop N times
2.      loop N times
3.           sleep 1 second</pre>
Line 3 runs in  [math]O(1)[/math] <em>constant time</em>, because one second is one second is one second. Line 3 is nested inside the loop defined by line 2, so it executes line 3 for N number of times. That comes out to <em>linear time of constant time</em> [math]O(n*1)[/math] The outer loop on line 1 executes the inner loop on line 2 for N times, which in turn runs the sleep function for N times, making the whole thing <em>linear time of linear time of constant time </em>[math]O(n*n*1)[/math] or just <em>polynomial time</em> [math]O(n^2)[/math].

By belaboring the analysis of this simple little function, we get a firm foundation to build on when things try to trip us up later.

<a title="Simple Complexity 4" href="http://stickybits.azurewebsites.net/?p=1631">Going a little deeper...</a>

&nbsp;
