<a title="Simple Complexity" href="http://stickybits.azurewebsites.net/?p=41">jump to beginning of series</a><a title="Simple Complexity 7" href="http://stickybits.azurewebsites.net/?p=4671"> or previous page</a>

<big>On the previous page, I said we won't dive into derivatives here. It depends upon the meaning of the word 'here' here. <strong>The derivative of a function is another function that describes the rate of change of the first function.</strong></big>

Plotting a function [math]f(x)[/math] on a cartesian plane, we might interpret the derivative of that function [math]f'(x)[/math] as a second function that quantifies the continuously changing slope of the curve of the first function.

Imagine using a pencil to draw the curve of function [math]f[/math] on a piece of graph paper. If you carefully note the exact direction you are pushing the pencil at each moment, you will have a new function uh, <em>derived from</em> the original function. By convention, the slope is quantified as rise over run, or how much upward change in pencil position is needed to keep pace with rightward movement of your pencil. If the pencil goes downward as it moves to the right, this is interpreted as a negative upward movement, and thus a negative slope. A very steep rise covers many rows of graph paper in just one column. The number of rows up per single column is the slope.  A function can never have a perfectly vertical or beyond vertical (back-tracking) slope because this would make the function not a function. We won't dive into that here.

So how does this relate to algorithmic complexity analysis? The short answer is that the higher the derivative at any given <em>n</em> value, the more resource is needed to make forward progress to the next <em>n</em> value; end of discussion. Read on for a longer answer.

Recall this heuristic for identifying time complexity:

When n is bigger by one, execution flow...
... adds no additional operation: [math]O(1)[/math] constant time
... adds one constant-time operation: [math]O(n)[/math] linear time
... adds one linear-time operation: [math]O(n^2)[/math] quadratic time
... adds one quadratic-time operation: [math]O(n^3)[/math] cubic time

Here is a plotting of the four functions defining those four time complexities: [math]1,\ n,\ n^2,\ n^3[/math]

<a href="http://stickybits.azurewebsites.net/wp-content/uploads/2014/12/visual_derivative.png"><img src="http://stickybits.azurewebsites.net/wp-content/uploads/2014/12/visual_derivative.png" alt="visual_derivative" width="625" height="440" class="aligncenter size-full wp-image-10181" /></a>



The exact derivatives of those four functions are [math]0,\ 1,\ 2n,\ 3n^2[/math] respectively. In big O, that would be [math]O(0),\ O(1),\ O(n),\ O(n^2)[/math] respectively. This is a demonstration of the principle that the derivative of a big-O function quantifies the additional operations of each increment of the n value. A quadratic-time operation is a linear-time operation, itself run in linear time: [math]O(n^2) = O(n) * O(n)[/math] If you identify a quadratic-time operation in your code that, through looping or recursion runs in linear time -- runs some number of times in proportion to your n value -- then taken together the whole piece of code runs in cubic time. Alternatively, if you have a linear-time operation run in quadratic time, you also have a cubic time complexity since multiplication is commutative. [math]O(n^2) * O(n) = O(n^2 * n) = O(n^3)[/math]

Continuous math is nice for smooth curve visualization, but discrete math is more appropriate to algorithmic analysis. Let's have a look at [math]f(n) = n^2[/math] represented as a bar graph.

<a href="http://stickybits.azurewebsites.net/wp-content/uploads/2014/12/discrete_nSquared.png"><img src="http://stickybits.azurewebsites.net/wp-content/uploads/2014/12/discrete_nSquared.png" alt="discrete_nSquared" width="600" height="435" class="aligncenter size-full wp-image-10191" /></a>

In continuous math, there is an infinite density of numbers; between any two numbers there are more numbers. In discrete math, we deal in integers. In this context, we might interpret a derivative of a function as the stepwise resolution as our <em>n</em> value varies by its smallest possible change, i.e. increments or decrements by one.

This visual representation of the quadratic function shows that for each increment of <em>n</em>, the computational load rises in linear fashion. Each new stripe wrapping around the inset square is longer than the previous stripe by the same two units. The formula of each line shown on the right quantifies the load associated with each increment of <em>n</em>. This has the effect of making the slope of each successive line steeper by a cumulative constant, creating a steepening parabolic function.

Let's take a quick look into the degenerate end of this progression. The derivative of [math]f(n) = 1[/math] is [math]f'(n) = 0[/math] So how can we interpret an [math]O(0)[/math] time complexity? Sticking with our inductive progression, we could say this zero-time operation not only does no more work for a bigger n, but does no work at all for any n. That is, an operation smaller than calling a function that immediately returns; smaller than a single no-op instruction; an operation too small to take any execution time at all. A perfectly zero-length operation. What's the derivative of [math]f(n) = 0[/math] ? Since zero is a constant, that would be [math]f'(0) = 0[/math] Stated in words, a linear-time operation does a constant-time operation per n value, a constant-time operation does a zero-time operation per n value, and a zero-time operation does exactly zero computational work in all circumstances. In a normal context, nobody would ever explicitly talk about an [math]O(0)[/math] zero-time operation, because it's always exactly the same null operation: a series of zero instructions that aren't even there. Mathematicians like to be complete and look at the degenerate case, so there you go. You're a mathematician.

So how can [math]O(n^2)[/math] represent a different complexity from [math]O(n^3)[/math] despite the fact that both 2 and 3 are constants? The short answer is that big O sums follow rules that differ from big O products: [math]O(n) + O(n) = O(n)[/math] while [math]O(n) * O(n) = O(n^2)[/math] This difference is revealed in the derivatives. [math]O(n) + O(n) + ... + O(n)[/math] for n number of terms (or a number of terms in proportion to n) equals [math]O(n)n = O(n^2)[/math] because it represents a linear number of linear-time operations. The derivative of [math]3n[/math] and the derivative of [math]n/5[/math] are both flatlines. But the quadratic and cubic functions do not point in the same direction. The derivative of [math]n^3[/math] is a parabolic function, and the derivative of [math]n^2[/math] is a rising line of slope 2. The bigger n gets, the further apart these derivatives diverge.

That's enough on derivatives. Let's look into divergence.

&nbsp;
