<a title="Simple Complexity" href="http://stickybits.azurewebsites.net/?p=41">jump to beginning of series</a><a title="Simple Complexity 7" href="http://stickybits.azurewebsites.net/?p=4671"> or previous page</a>

<big>On the previous page, I said we won't dive into derivatives here. It depends upon the meaning of the word 'here' here. <strong>Don't panic.</strong> You may not absorb everything on this page. This math discussion is here to provide insight into how complexity builds on itself. Read on to gain whatever value you can, but don't get bogged down. We'll get back to the code soon enough.</big>

The derivative of a function is another function that describes the rate of change of the first function. Plotting a function [math]f(x)[/math] on a cartesian plane, we might interpret the derivative of that function [math]f'(x)[/math] as a second function that quantifies the continuously changing slope of the curve of the first function.

Imagine using a pencil to plot a function on a piece of graph paper. If you carefully note the exact direction you are pushing the pencil at each moment, you will have a new function derived from the original function. By convention, the slope is quantified as rise over run, or how much upward change in pencil position is needed to keep pace with rightward movement of your pencil. If the pencil goes downward as it moves to the right, this is interpreted as a negative upward movement, and thus a negative slope. A very steep rise covers many rows of graph paper in just one column. The number of rows upward per single column rightward is the slope. A curve has a constantly changing slope, so the slope at any one point of that curve is defined by the linear direction the pencil is being pushed at that moment. The word 'curve' might be used to describe a function plotting even if it's a straight line, just to be confusing. Think of a line as the kind of curve that doesn't change direction. A function can never have a perfectly vertical or beyond vertical (overhanging) slope because this would make the function not a function. We won't dive into that here.

So how does this relate to algorithmic complexity analysis? The short answer is that the higher the derivative at any given <em>n</em> value, the more resource is needed to make forward progress to the next <em>n</em> value; end of discussion. Read on for a longer answer.

Recall this heuristic for identifying time complexity:

When n is bigger by one, execution flow...
... adds no additional operation: [math]O(1)[/math] constant time
... adds one constant-time operation: [math]O(n)[/math] linear time
... adds one linear-time operation: [math]O(n^2)[/math] quadratic time
... adds one quadratic-time operation: [math]O(n^3)[/math] cubic time

Here is a plotting of the four functions defining those four time complexities: [math]1,\ n,\ n^2,\ n^3[/math]

<a href="http://stickybits.azurewebsites.net/wp-content/uploads/2014/12/visual_derivative.png"><img src="http://stickybits.azurewebsites.net/wp-content/uploads/2014/12/visual_derivative.png" alt="visual_derivative" width="625" height="440" class="aligncenter size-full wp-image-10181" /></a>

This figure demonstrates a progression of derivatives. Have a look at the stair steps along the linear function's blue line. Each step forward along the linear function requires a proportional step upward to stay with the line. The blue line rises at a constant rate with respect to n. Those stairs grow taller at the same rate that the gold constant function rises: which is to say, they stay the same forever. With the quadratic function in violet, each step on the staircase is taller than the previous step, and that change in height grows at a constant rate. <em>"I know an old lady who swallowed a fly..."</em> The parabola rises at a rate that rises at a rate that doesn't change. The green cubic function rises at a rate that rises at a rate that rises at a rate that doesn't change.

The exact derivatives of those four functions are [math]0,\ 1,\ 2n,\ 3n^2[/math] respectively. In big O, that would be [math]O(0),\ O(1),\ O(n),\ O(n^2)[/math] respectively. Those are the original functions shifted by one position. This represents the principle that the derivative of a big-O function quantifies the additional operations of each increment of the n value. A quadratic-time operation is a linear-time operation, itself run in linear time: [math]O(n^2) = O(n) * O(n)[/math] If you identify a quadratic-time operation in your code that, through looping or recursion runs in linear time -- runs some number of times in proportion to your n value -- then taken together the whole piece of code runs in cubic time. Alternatively, if you have a linear-time operation run in quadratic time, you also have a cubic time complexity since multiplication is commutative. [math]O(n^2) * O(n) = O(n^2 * n) = O(n^3)[/math]

Continuous math is nice for smooth curve visualization, but discrete math is more appropriate to algorithmic analysis. Let's see [math]f(n) = n^2[/math] as a bar graph.

<a href="http://stickybits.azurewebsites.net/wp-content/uploads/2014/12/discrete_nSquared.png"><img src="http://stickybits.azurewebsites.net/wp-content/uploads/2014/12/discrete_nSquared.png" alt="discrete_nSquared" width="600" height="435" class="aligncenter size-full wp-image-10191" /></a>

In continuous math, there is an infinite density of numbers; between any two numbers there are more numbers. In discrete math, we deal in integers. In this context, we might interpret a derivative of a function as the stepwise resolution as our <em>n</em> value varies by its smallest possible change, i.e. increments or decrements by one.

This visual representation of the quadratic function shows that for each increment of <em>n</em>, the computational load rises in linear fashion. When you have an n-loop nested within an n-loop, the innermost code is run in proportion to [math]n^2[/math] number of times. A slightly bigger n value causes the inner loop to iterate slightly longer, and to be run slightly more times. This is represented geometrically by the inset square. Each successive stripe wrapping around the inset square is longer than the previous stripe by two units, one for each dimensional aspect. The formula of each line shown on the right quantifies the load associated with each increment of <em>n</em>. This has the effect of making the slope of each successive line steeper by a cumulative constant, creating a steepening parabolic function.

Let's take a quick look into the degenerate end of this progression. The derivative of [math]f(n) = 1[/math] is [math]f'(n) = 0[/math] So how can we interpret an [math]O(0)[/math] time complexity? Sticking with our inductive progression, we could say this zero-time operation not only does no more work for a bigger n, but does no work at all for any n. That is, an operation smaller than invoking a function that immediately returns; smaller than a single no-op instruction; an operation too small to take any execution time at all. A perfectly zero-length operation. What's the derivative of [math]f(n) = 0[/math] ? Since zero is a constant, that would be [math]f'(0) = 0[/math] Stated in words, a zero-time operation does exactly zero computational work in all circumstances. In a normal context it's silly to talk about an [math]O(0)[/math] zero-time operation, because it's always exactly the same null operation: a series of zero instructions that aren't even there. Mathematicians like to be complete and look at the degenerate case, so there you go. You're a mathematician.

So here's how [math]O(n^2)[/math] can represent a different complexity from [math]O(n^3)[/math] despite the fact that both 2 and 3 are constants. Big O sums behave differently from big O products. [math]O(n) + O(n) + ... + O(n)[/math] for fifty terms is equal to [math]O(50n) = O(n)[/math] because no matter how big your <em>n</em> grows, the number fifty always stays the same. Doubling n from a big value doubles the resolution of the expression. On the other hand, [math]O(n) + O(n) + ... + O(n)[/math] for <em>n</em> number of terms (or [math]n/50[/math] or [math]n/1000[/math] terms) is equal to [math]O(n^2)[/math] because the number of terms grows in proportion to the <em>n</em> value. Doubling n from a big value quadruples the resolution of the expression.

[math]O(n) * O(n) = O(n^2)[/math] represents a linear number of linear-time operations. The derivative of [math]50n[/math] and the derivative of [math]n/1000[/math] are both flatlines -- they're parallel lines. As n grows, they stay side-by-side. The derivative of [math]n^3[/math] is a parabola and the derivative of [math]n^2[/math] is a line. The bigger n gets, the further apart these derivative functions spread. In other words, as n grows the original functions grow less parallel, and thus more different.

In this context, the underlying meaning of the derivative is a description of how much work needs to be done per n. This page gives another perspective on the recurring theme that a quadratic-time operation must do a linear-time operation per n, a linear operation must do a constant-time operation per n, and a constant-time operation must do a zero-time operation per n. But this can take us far beyond the simple [math]O(n^k)[/math] formulation. Using derivatives we can unravel the most heinous big O functions to get a simpler complexity profile that itself runs in linear time. And when we have multiple independent variables in our big O, we can take the derivatives with respect to each of them to see the time complexity with respect to each linear vector.

That's enough on derivatives. If any of that stuck, you're doing alright. If you don't really care about this page, you could've skipped it. If you think this page is awesome, then I think you are awesome. Now let's turn toward convergence.

&nbsp;
