<a title="Simple Complexity" href="http://stickybits.azurewebsites.net/?p=41">jump to beginning of series</a><a title="Simple Complexity 5" href="http://stickybits.azurewebsites.net/?p=4671"> or previous page</a>

<big>On the previous page, I said we won't dive into derivatives here. It depends upon the meaning of the word 'here' here. <strong>The derivative of a function is another function that describes the rate of change of the first function.</strong> Plotting a function [math]f(x)[/math] on a cartesian plane, we might interpret the derivative of that function [math]f'(x)[/math] as a second function that quantifies the continuously changing slope of the curve of the first function.</big>

Imagine using a pencil to draw the curve of function <em>f</em> on a piece of graph paper taped to a tabletop. Holding the pencil by the eraser end you lay a chopstick down on the table and carefully push the pencil's writing tip along the graph paper by the pointy end of the chopstick. If you define a new function to plot the direction of the chopstick at each point in the drawing, that new function <em>f'[math] is the derivative of the first function. By convention, the slope is quantified as rise over run, or how much upward change in pencil position is needed to keep pace with rightward movement of your pencil. If the pencil goes downward as it moves to the right, this is interpreted as a negative upward movement, and thus a negative slope. A function can never have a perfectly vertical or beyond vertical (back-tracking) slope because this would make the function not a function. We won't dive into that here.

So how does this relate to algorithmic complexity analysis? The short answer is that the higher the derivative at any given [em]n[/em] value, the more resource is needed to make forward progress to the next [em]n[/em] value; end of discussion.

Here's the slightly longer answer. Recall this heuristic for identifying time complexity:

When n is bigger by one, execution flow...
... adds no additional operation: [math]O(1)[/math] constant time
... adds one constant-time operation: [math]O(n)[/math] linear time
... adds one linear-time operation: [math]O(n^2)[/math] quadratic time
... adds one quadratic-time operation: [math]O(n^3)[/math] cubic time

Here is a plotting of the four functions defining those four time complexities: [math]1[/math] , [math]n[/math] , [math]n^2[/math] , [math]n^3[/math]



The derivatives of those four functions are [math]0[/math] (the <em>n</em> axis), [math]1[/math] , [math]2n[/math] , [math]3n^2[/math] , respectively. Each of those derivatives is one of the original functions, multiplied by a constant. This is another way of looking at complexity. The derivative of a big-O function quantifies the additional complexity of each additional n value. A quadratic-time operation is a linear-time operation, itself run in linear time. 




If you identify a quadratic-time operation in your code that, through looping or recursion, runs in linear time -- runs some number of times in proportion to your n value -- then taken together the whole piece of code runs in cubic time. Alternatively, if you have a linear-time operation run in quadratic time, you also have a cubic time complexity since multiplication is commutative. [math]O(n^2) * O(n) = O(n * n^2) = O(n^3)[/math]

Continuous math is nice for smooth curve visualization, but discrete math is nice for algorithmic analysis visualization.





So how can [math]O(n^2)[/math] represent a different complexity from [math]O(n^3)[/math] despite the fact that both 2 and 3 are constants? In the same way that 

Let's take a quick look into the degenerate end of this progression. The derivative of [math]f(n) = 1[/math] is [math]f'(n) = 0[/math] So how can we interpret an O(0) time complexity? Sticking with our inductive progression, we could say this zero-time operation not only does no more work for a bigger n, but does no work at all for any n. That is, an operation smaller than calling a function that immediately returns; smaller than a single no-op instruction; an operation too small to take any execution time at all. A perfectly zero-length operation. What's the derivative of [math]f(n) = 0[/math] ? Since zero is a constant, that would be [math]f'(0) = 0[/math] Stated in words, a linear-time operation does a constant-time operation per n value, a constant-time operation does a zero-time operation per n value, and a zero-time operation does exactly zero computational work in all circumstances. In a normal context, nobody would ever explicitly talk about an [math]O(0)[/math] zero-time operation, because it's always exactly the same null operation: a series of zero instructions that aren't even there. Mathematicians like to be complete and look at the degenerate case, so there you go. You're a mathematician.

Now let's look at why the stub terms are disregarded

 in complexity analysis.

&nbsp;
